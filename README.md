# RAG Content Ingestion Pipeline

A modular pipeline for scraping, parsing, and processing content into JSONL suitable for Retrieval-Augmented Generation (RAG) ingestion.

## Features

- ðŸŒ URL scraping (HTML snapshots, main content extraction, PDF detection)
- ðŸ“„ PDF parsing (via `pdfplumber`)
- ðŸ’¾ Local caching (`cache/raw` for raw HTML/PDF text)
- ðŸ§  Sliding window processing with DeepSeek (`deepseek-reasoner`)
- âœ… Deduplication & JSONL output (`cache/rag_ready`)
- ðŸ”§ Modular design (`rag_pipeline/` package with submodules)

### Coming Soon
- â˜ï¸ GCS storage integration
- ðŸ“¦ Automated Pinecone ingestion
- â° Cloud Scheduler for periodic refresh

---

## Pipeline Flow

---

## Quick Start Flow

```mermaid
flowchart LR
    A[URLs] --> B[Scraper/PDF Parser] --> C[cache/raw]
    C --> D[Sliding Window + DeepSeek] --> E[cache/rag_ready JSONL]
```

## Detailed Pipeline Flow

```mermaid
flowchart TD
    A[URL list<br/>config/urls.txt] --> B[Scraper<br/>HTML/PDF detect]
    B -->|Save raw HTML| C[cache/raw]
    B -->|Download PDFs| D[PDF Parser]
    D -->|Save raw text| C

    C --> E[Sliding Window Parser<br/>DeepSeek API]
    E -->|Deduplicate + Clean| F[cache/rag_ready<br/>JSONL]

    F --> G[(RAG ingestion<br/>Vector DB / Pinecone)]
```

---

## Project Structure

```text
.
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ raw/           # raw scraped HTML/PDF text
â”‚   â””â”€â”€ rag_ready/     # processed JSONL output
â”œâ”€â”€ config/
â”‚   â””â”€â”€ urls.txt       # list of target URLs
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ rag_pipeline/
    â”œâ”€â”€ cli.py
    â”œâ”€â”€ main.py
    â”œâ”€â”€ scraping/
    â”‚   â”œâ”€â”€ scraper.py
    â”‚   â””â”€â”€ pdf_parser.py
    â”œâ”€â”€ processing/
    â”‚   â”œâ”€â”€ ai_client.py
    â”‚   â””â”€â”€ sliding_window.py
    â”œâ”€â”€ storage/
    â”‚   â””â”€â”€ storage.py
    â””â”€â”€ utils/
        â””â”€â”€ logger.py
```

---

## Setup

1. Clone the repo
2. Create `.env` with your DeepSeek key:
   ```env
   DEEPSEEK_API_KEY=your_key_here
   ```
3. Build the image:
   ```bash
   docker-compose build
   ```

---

## Usage

### Interactive CLI
Run the CLI to select a URL or run all:

```bash
docker-compose run --rm
```

### Direct Orchestration
Run the whole pipeline on all URLs in `config/urls.txt`:

```bash
docker-compose run --rm python -m rag_pipeline.main
```

Run it on a single URL:

```bash
docker-compose run --rm python -m rag_pipeline.main https://example.com/page
```

---

## Example Output

Example JSONL (`cache/rag_ready/irb_manual.jsonl`):

```json
{"text": "Informed consent requires disclosure of risks and benefits..."}
{"text": "Investigators must maintain accurate and complete study records..."}
{"text": "IRB review ensures compliance with federal regulations..."}
```
